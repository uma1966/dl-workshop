{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Deep Learning\n",
    "\n",
    "Dieses Tutorial zeigt, wie man mittels eines Neuronalen Netzwerks / Deep Learning einen Bild-Klassifizierer baut, der Katzenbilder von Hundebildern unterscheidet.\n",
    "\n",
    "Diese Aufgabenstellung kommt aus dem [\"Cats vs. Dogs\"](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) Wettbewerb der Website Kaggle.\n",
    "\n",
    "Basis bildet ein Convolutional Neuronal Network (CNN) namens \"VGG16\", welches auf Basis der Daten des [Imagenet Datasets](http://image-net.org/synset?wnid=n02084071) vortrainiert wurde. Das Modell wird durch Umkonfiguration und Re-Training so angepasst, dass es die gestellte Aufgabe lösen kann.\n",
    "\n",
    "Die Grundlagen zu diesem Workshop kommen aus dem Deep Learning MOOC [fast.ai](http://fast.ai).\n",
    "\n",
    "## Inhalt\n",
    "\n",
    "1. [Data preparation](#Data-preparation) - Welche Daten verarbeiten wir?\n",
    "1. [Setup](#Setup) - Initialisierung der notwendigen Frameworks und des Modells\n",
    "1. [Training](#Training) - Training des Modells mit unseren Daten\n",
    "1. [Vorhersage](#Vorhersage) - Das trainierte Modell anwenden, um eine Vorhersage zu treffen\n",
    "1. [Visualisieren der Ergebnisse](#Visualisieren-der-Ergebnisse) - Wir schauen uns an, welche Ergebnisse das Modell liefert\n",
    "1. [Mehr Layer trainieren](#Mehr-Layer-trainieren) - Das Training intensivieren\n",
    "1. [Exkurs: Wenige Bilder Trainieren, viele Bilder klassifizieren](#Exkurs:-Wenige-Bilder-Trainieren,-viele-Bilder-klassifizieren)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten des Kaggle Wettbewerbs wurden schon vorbereitet und in der \"richtigen\" Struktur abgelegt.\n",
    "Das Verzeichnis `data` enthält die Trainings- und Validierungsdaten aus dem Dataset. Dabei sind die Bilder zu jeder zu erkennenden \"Klasse\" (Cats & Dogs in unserem Fall) in einem eigenen Unterverzeichnis abgelegt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! führt einen Shell-Befehl aus...\n",
    "!tree -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mal schauen, wieviele Dateien in den Trainings- und Validerungsdaten drin sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo -n \"Training cats: \" && ls data/train/cats | wc -w\n",
    "!echo -n \"Training dogs: \" && ls data/train/dogs | wc -w\n",
    "!echo -n \"Validation cats: \" && ls data/valid/cats | wc -w\n",
    "!echo -n \"Validation dogs: \" && ls data/valid/dogs | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Verzeichnis `test` enthält die Bilder, die nicht klassifizert sind (deshalb das Unterverzeichnis `unknown`). Diese wollen wir nach dem Training bestimmen. Mal sehen, wieviele das sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo -n \"Test: \" && ls data/test/unknown | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir am Code herumprobieren können, ohne gleich lange Laufzeiten aufgrund der vielen Dateien zu erhalten, gibt es noch ein `sample` Dataset, welches gleich aufgebaut ist, aber nur nur einen kleinen Teil der Daten enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tree -d sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo -n \"Training cats: \" && ls sample/train/cats | wc -w\n",
    "!echo -n \"Training dogs: \" && ls sample/train/dogs | wc -w\n",
    "!echo -n \"Validation cats: \" && ls sample/valid/cats | wc -w\n",
    "!echo -n \"Validation dogs: \" && ls sample/valid/dogs | wc -w\n",
    "!echo -n \"Test: \" && ls sample/test/unknown | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Diesen Teil immer ausführen. Hier werden notwendige Packete geladen und globale Variablen initialisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "import shutil\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "import utils\n",
    "import keras\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier setzen wir den Pfad für die Daten, mit denen wir arbeiten wollen (also `data` oder `sample`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path = \"data/\"\n",
    "path = \"sample/\"\n",
    "path = os.path.join(os.path.curdir,path)\n",
    "print path\n",
    "\n",
    "train_path = os.path.join(path,\"train\")\n",
    "valid_path = os.path.join(path,\"valid\")\n",
    "test_path = os.path.join(path,\"test\")\n",
    "result_path = os.path.join(path,\"results\")\n",
    "\n",
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=64\n",
    "\n",
    "print train_path\n",
    "print valid_path\n",
    "print test_path\n",
    "print result_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden die Python Klasse, welche das hier verwendete Modell in ein nettes, mehr oder weniger objektorientiertes API verpackt. Der Sourcecode dazu steht in der Datei `vgg16.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import VGG16 class, and instantiate\n",
    "import vgg16\n",
    "vgg = vgg16.Vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das VGG16 Modell ist ein Convolutional Neuronal Network (CNN) und wurde an der University of Oxford von der [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/) entwickelt und veröffentlicht. Es wurde verwendet um den [Imagenet Contest](http://image-net.org/synset?wnid=n02084071) zu gewinnen. Es erkennt 1000 verschiedene Ojekte (= es liefert zu einem Bild 1000 Wahrscheinlichkeitswerte, ob das Bild ein Ding dieser Klasse enthält).\n",
    "\n",
    "Wir können einen Blick in den Code der Klasse werfen (?? zeigt die Implementierung eines Code-Elements):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können auch einen Blick auf die Struktur des Modells werfen. Der folgende Befehl zeigt die Schichten (\"Layer\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden die Trainings- und Validierungsdaten als \"Batches\". Ein Batch liefert immer die nächsten _n_ Datensätze inklusive der Kategorie. So wird beim Training über die gesamte Menge der Trainings- und Validierungsdaten iteriert.\n",
    "\n",
    "`vgg.get_batches()` erwartet, dass die Daten in Unterverzeichnissen je Kategorie abgelegt sind. Genau das ist bei uns der Fall, wie wir oben gesehen haben (Verzeichnisse \"cats\" und \"dogs\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batches = vgg.get_batches(train_path, batch_size=batch_size)\n",
    "validation_batches = vgg.get_batches(valid_path, batch_size=batch_size*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir auch sehen, hat der Befehl automatisch erkannt, wie viele Klassen in unseren Trainings-/Validierungsdaten enthalten sind: `2 classes` (cats & cogs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann wird das VGG16 Modell an unsere Aufgabe (\"cat or dog\" Klassifizierung) angepasst mit `vgg.finetune()`. Dies ändert die Architektur des Netzes: Der letzte Layer wird verworfen und durch einen neuen Layer erstetzt, welcher nur noch 2 Outputs hat (statt wie bisher 1000): Cats & Dogs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.finetune(train_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns anschauen, was die Methode `finetune` macht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg.finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der eigentliche Austausch des letzten Layers erfolgt in der Methode `vgg.ft()`. Die Gewichte des neuen Layers sind zunächst mit Zufallswerten initialisiert worden, d.h. sie müssen noch trainiert werden. Die anderen Layer lassen wir, wie sie sind (`layer.trainable = False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg.ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt trainieren wir das Modell mit den Daten über `vgg.fit()`. Dabei wird in Wahrheit nur noch der letze (modifizierte) Layer des angepassten VGG16 Modells trainiert. Die angepassten Gewichte des Modells schreiben wir in eine Datei, so dass wir sie später wieder laden können und so nicht jedesmal das Training wiederholen müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rate:\n",
    "vgg.model.optimizer.lr = 0.01\n",
    "\n",
    "print 'start fitting at ' + time.asctime()\n",
    "\n",
    "vgg.fit(train_batches, validation_batches, nb_epoch=1)\n",
    "\n",
    "print 'stop fitting at ' + time.asctime()\n",
    "\n",
    "weights_filename = os.path.join(result_path,'finetune1.h5')\n",
    "print 'saving weights to ' + weights_filename\n",
    "vgg.model.save_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage\n",
    "\n",
    "Jetzt klassifizieren wir mit dem re-trainierten Modell die Bilder, die im Unterverzeichnis 'test' abgelegt sind. Anders ausgedrückt: Wir sagen für eine Menge Daten (= ein Bild) vorher, mit jeweils welcher Wahrscheinlichkeit diese Daten eine Katze bzw. ein Hund ist. Die Ergebnisse speichern wir wieder in Dateien, damit wir später bei Bedarf darauf zugreifen können, ohne das ganze Training wiederholen zu müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dateien zum Speichern der Ergebnisse:\n",
    "predictions_file = os.path.join(result_path,'predictions.dat')\n",
    "filenames_file = os.path.join(result_path,'filenames.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction durchführen und Ergebnisse speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'start predicting at ' + time.asctime()\n",
    "test_batches, predictions = vgg.test(test_path,batch_size=batch_size*2)\n",
    "print 'stop predicting at '+ time.asctime()\n",
    "\n",
    "filenames = test_batches.filenames\n",
    "utils.save_array(predictions_file, predictions)\n",
    "utils.save_array(filenames_file, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mal ein paar Ergebnisse anschauen\n",
    "\n",
    "Zunächst eine kleine Hilfsmethode, um Bilder anzeigen zu können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def plots_idx(idx, path, filenames, titles=None):\n",
    "    \"\"\"Loads and displays images with given titles. The images are given with their index in filenames.\"\"\"\n",
    "    utils.plots([image.load_img(os.path.join(path,filenames[i])) for i in idx], titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden die Ergebnisse aus den oben geschriebenen Dateien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = utils.load_array(predictions_file)\n",
    "filenames = utils.load_array(filenames_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wählen zufällig ein paar Bilder aus und zeigen sie mit der Vorhersage an (`[Wahrscheinlichkeit_Katze, Wahrscheinlichkeit_Hund]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(test_batches.filenames),4)\n",
    "plots_idx(idx, path=test_path, filenames=test_batches.filenames, titles=predictions[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisieren der Ergebnisse\n",
    "\n",
    "Wir wollen uns anschauen, wie gut unser Modell eigentlich vorhersagt. Die Idee dazu ist, dass wir mit dem Modell eine Vorhersage über die bereits klassifizierten Trainingsdaten machen. So kennen wir die \"ground truth\" zu jedem Bild und können ermitteln, ob die Vorhersage korrekt war."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den folgenden Block muss man nur ausführen, wenn man das Training oben bereits vorher mal gemacht hatte und die Gewichte in eine Datei geschrieben hat. Dann kann man hier direkt das Modell neu laden mit den (veränderten) Gewichten. Hat man in der gleichen Sitzung das Training schon gemacht, kann man diesen Schritt überspringen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren (falls wir hier wieder beginnen wollen):\n",
    "import vgg16\n",
    "vgg = vgg16.Vgg16()\n",
    "# Nicht vergessen, die letzte Schicht zu verändern (2 statt 1000 Klassen als Output)!\n",
    "vgg.finetune(train_batches)\n",
    "# Gewichte laden:\n",
    "weights_filename = os.path.join(result_path,'finetune1.h5')\n",
    "vgg.model.load_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhersage mit den Validierungsdaten. So kennnen wir die \"ground truth\" und können sie mit der Vorhersage des Modells vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vorhersage machen:\n",
    "valid_batches, predictions = vgg.test(valid_path, batch_size=64)\n",
    "\n",
    "print \"First n predictions:\"\n",
    "print predictions[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir stutzen uns das Ergebnis zurecht, so dass wir nur noch ein 1-dimeansionales Array mit einer `1` für einen vorhergesagten Hund und einer `0` für eine vorgesagte Katze haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "our_predictions = predictions[:,1]\n",
    "print our_predictions[:8]\n",
    "our_labels = np.round(our_predictions)\n",
    "print \"Prediction, if it's a dog: \", our_labels[:8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist unsere \"ground truth\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_labels = valid_batches.classes\n",
    "filenames = valid_batches.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeige einige korrekte Klassifizierungen\n",
    "\n",
    "Mit Numpy (`np`) können wir wir den Index aller Bilder ermitteln, bei denen unser vorhergesagtes Label (0 oder 1) mit dem erwarteten Label der \"groud truth\" übereinstimmt. Davon wählen wir zufällig 4 aus und zeigen sie an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = np.where(our_labels==expected_labels)[0]\n",
    "print \"Found {} correct labels\".format(len(correct))\n",
    "idx = np.random.permutation(correct)[:4]\n",
    "titles = [filenames[i]+'\\n'+ str(our_labels[i]) for i in idx]\n",
    "plots_idx(idx, valid_path, filenames, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zeige einige falsche Klassifizierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incorrect = np.where(our_labels!=expected_labels)[0]\n",
    "print \"Found {} incorrect labels.\".format(len(incorrect))\n",
    "if len(incorrect)>0:\n",
    "    idx = np.random.permutation(incorrect)[:4]\n",
    "    titles = [filenames[i]+'\\n'+ str(our_labels[i]) for i in idx]\n",
    "    plots_idx(idx, valid_path, filenames, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeige einige richtige Klassifizierungen mit großer  Wahrscheinlichkeit\n",
    "\n",
    "... also die, bei denen das Modell wirklich recht hatte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_view = 4\n",
    "\n",
    "# The images we most confident were dogs, and are actually dogs\n",
    "correct_dogs = np.where((our_labels==1) & (our_labels==expected_labels))[0]\n",
    "print \"Found {} confident correct dogs labels\".format(len(correct_dogs))\n",
    "most_correct_dogs = np.argsort(our_predictions[correct_dogs])[::-1][:n_view]\n",
    "plots_idx(correct_dogs[most_correct_dogs], valid_path, filenames, our_predictions[correct_dogs][most_correct_dogs])\n",
    "\n",
    "# The images we most confident were cats, and are actually cats\n",
    "correct_cats = np.where((our_labels==0) & (our_labels==expected_labels))[0]\n",
    "print \"Found {} confident correct cats labels\".format(len(correct_cats))\n",
    "most_correct_cats = np.argsort(our_predictions[correct_cats])[::][:n_view]\n",
    "plots_idx(correct_cats[most_correct_cats], valid_path, filenames, our_predictions[correct_cats][most_correct_cats])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeige einige falsche Klassifizierungen mit der großer Wahrscheinlichkeit\n",
    "\n",
    "... also die, bei denen das Modell total daneben lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The images we most confident were dogs, and are actually dogs\n",
    "configent_dogs = np.where((our_labels==1) & (our_labels!=expected_labels))[0]\n",
    "print \"Found {} confident incorrect dogs labels\".format(len(confident_dogs))\n",
    "if len(confident_dogs)>0:\n",
    "    most_confident_dogs = np.argsort(our_predictions[confident_dogs])[::-1][:n_view]\n",
    "    plots_idx(confident_dogs[most_confident_dogs], valid_path, filenames, our_predictions[confident_dogs][most_confident_dogs])\n",
    "\n",
    "#threshold = 0.9\n",
    "#confident_dogs = np.where((our_labels!=expected_labels) & (our_labels==1) & (our_predictions>threshold))[0]\n",
    "#print \"Found {} confident incorrect dogs labels\".format(len(confident_dogs))\n",
    "#if len(confident_dogs)>0:\n",
    "#    idx = np.random.permutation(confident_dogs)[:4]\n",
    "#    titles = [str(our_predictions[i])+'\\n'+filenames[i] for i in idx]\n",
    "#    plots_idx(idx, valid_path, filenames, titles)\n",
    "\n",
    "#threshold = 0.0001\n",
    "#confident_cats = np.where((our_labels!=expected_labels) & (our_labels==0) & (our_predictions<threshold))[0]\n",
    "#print \"Found {} confident incorrect cats labels\".format(len(confident_cats))\n",
    "#if len(confident_cats)>0:\n",
    "#    idx = np.random.permutation(confident_cats)[:4]\n",
    "#    titles = [str(our_predictions[i])+'\\n'+filenames[i] for i in idx]\n",
    "#    plots_idx(idx,valid_path, filenames, titles)\n",
    "\n",
    "# The images we most confident were dogs, and are actually dogs\n",
    "configent_cats = np.where((our_labels==0) & (our_labels!=expected_labels))[0]\n",
    "print \"Found {} confident incorrect cats labels\".format(len(confident_cats))\n",
    "if len(confident_cats)>0:\n",
    "    most_confident_cats = np.argsort(our_predictions[confident_cats])[::][:n_view]\n",
    "    plots_idx(confident_cats[most_confident_cats], valid_path, filenames, our_predictions[confident_cats][most_confident_cats])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ziege die unsichersten Klassifizierungen\n",
    "\n",
    "... also die, bei denen sich das Modell nicht so sicher war."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncertain = np.argsort(np.abs(our_predictions-0.5))\n",
    "print our_predictions\n",
    "print uncertain\n",
    "#titles = [str(our_predictions[i])+'\\n'+filenames[i] for i in uncertain]\n",
    "plots_idx(uncertain[:6], valid_path, filenames, our_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels,our_labels)\n",
    "utils.plot_confusion_matrix(cm, valid_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mehr Layer trainieren\n",
    "\n",
    "Bisher haben wir nur den letzten Layer des Modells neu trainiert und den Rest des Modells nicht angetastet. Wenn wir nun auch die mittleren Layer re-trainieren wollen geht das mit Keras ziemlich einfach...\n",
    "\n",
    "Zunächst: Modell laden und initalisieren, indem wir die Gewichte aus unserem Training oben laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren:\n",
    "vgg = vgg16.Vgg16()\n",
    "# Nicht vergessen, die letzte Schicht zu verändern (2 statt 1000 Klassen als Output)!\n",
    "vgg.finetune(train_batches)\n",
    "# Gewichte aus Datei laden:\n",
    "weights_filename = os.path.join(result_path,'finetune1.h5')\n",
    "vgg.model.load_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nochmal kurz einen Blick auf die Struktur des Modells werfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achtung: Wir müssen darauf achten, den letzten Layer (den wir oben selbst hinzugefügt haben) vorher über `vgg.fit()` auch trainiert zu haben, da er sonst mit Zufallswerten initialisert ist, welche das Training der Zwischenschichten ziemlich durcheinander bringen würde. Wenn wir die zuvor in einer Datei gespeicherten Gewichte verwenden ist das automatisch der Fall.\n",
    "\n",
    "\n",
    "### Alle Dense Layer neu trainieren\n",
    "Der erste Versuch ist, nur die `Dense` Layer am Ende des Modells neu zu trainieren. `Dense` Layer bilden Lineare Funktionen ab, die mit allen Outputs der vorigen Layer verbunden sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hilfsmethode, um Modell anzupassen (fitting):\n",
    "def fit_model(model, train_batches, validation_batches, nb_epoch=1):\n",
    "    model.fit_generator(train_batches, samples_per_epoch=train_batches.N, nb_epoch=nb_epoch, \n",
    "                        validation_data=validation_batches, nb_val_samples=validation_batches.N)\n",
    "    \n",
    "# Hole den Index des ersten \"dense\" layers:\n",
    "first_dense_idx = [index for index,layer in enumerate(vgg.model.layers) if type(layer) is keras.layers.core.Dense][0]\n",
    "print \"First dense layer is layer no. \" + str(first_dense_idx)\n",
    "# ...und setze diesen und alle nachfolgenden auf \"trainierbar\":\n",
    "for layer in vgg.model.layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt trainieren wir _alle_ Layer ab dem ersten \"Dense\" Layer neu (diesmal mit 3 Durchläufen durch die Trainingsdaten) und speichern die Gewichte wieder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.backend.set_value(vgg.model.optimizer.lr, 0.01)\n",
    "weights_filename = os.path.join(result_path,'finetune2.h5')\n",
    "\n",
    "fit_model(vgg.model, train_batches, validation_batches, 3)\n",
    "\n",
    "vgg.model.save_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Modell zu beurteilen, machen wir wieder eine Vorhersage über ein klassifziertes Dataset (\"ground truth\") und berechnen eine Cross Entropy Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_and_plot_confusion_matrix(vgg, path):\n",
    "    # Prediction:\n",
    "    batches, predictions = vgg.test(path, batch_size=64)\n",
    "    our_predictions = predictions[:,0]\n",
    "    our_labels = np.round(1-our_predictions)\n",
    "\n",
    "    # Ground truth:\n",
    "    expected_labels = batches.classes\n",
    "\n",
    "    cm = sklearn.metrics.confusion_matrix(expected_labels,our_labels)\n",
    "    utils.plot_confusion_matrix(cm, batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_and_plot_confusion_matrix(vgg,valid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noch mehr Layer trainieren\n",
    "\n",
    "Wir können auch versuchen, noch mehr Layer zu trainieren (nicht nur die Dense-Layer am hinteren Ende des Networks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in vgg.model.layers[12:]: \n",
    "    layer.trainable=True\n",
    "\n",
    "keras.backend.set_value(vgg.model.optimizer.lr, 0.001)\n",
    "model_file = os.path.join(result_path,'finetune3.h5')\n",
    "\n",
    "fit_model(vgg.model, train_batches, validation_batches, 4)\n",
    "\n",
    "vgg.model.save_weights(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier berechnen wir wieder eine Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_and_plot_confusion_matrix(vgg,valid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exkurs: Wenige Bilder Trainieren, viele Bilder klassifizieren\n",
    "\n",
    "Wie gut ist unser Modell eigentlich, wenn wir nur mit wenigen Bildern trainieren?\n",
    "\n",
    "Vorgehensweise:\n",
    "* Trainieren mit den Bildern des `sample` Datasets (200 Bilder)\n",
    "* Vorhersagen mit den Bildern des normalen Datasets (23000 Trainingsbilder als Ground Truth)\n",
    "\n",
    "Da wir hier nicht das ganze Training wiederholen wollen, laden wir die Gewichte aus dem `sample` Pfad - das setzt voraus, dass man den Trainings-Code oben auch mal mit dem `sample` Dataset ausgeführt hat und somit die Datei mit den Gewichten existiert. Im Code unten kann man festlegen, welche Gewichte wir verwenden wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren (falls wir hier wieder beginnen wollen):\n",
    "vgg = vgg16.Vgg16()\n",
    "# Nicht vergessen, die letzte Schicht zu verändern (2 statt 1000 Klassen als Output)!\n",
    "vgg.finetune(train_batches)\n",
    "\n",
    "# Gewichte laden (aus dem 'sample' Pfad!)\n",
    "#weights = 'finetune1.h5'   # Nur der letzte Layer wurde trainiert\n",
    "#weights = 'finetune2.h5'   # Alle Dense Layer wurden trainiert\n",
    "weights = 'finetune3.h5'   # Alle Layer ab Nr. 12 wurden trainiert\n",
    "weights_filename = os.path.join('sample','results',weights)\n",
    "\n",
    "print \"using weights \" + weights_filename\n",
    "vgg.model.load_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Jetzt führen wir die Vorhersage mit dem \"normalen\" Dataset durch:\n",
    "predict_and_plot_confusion_matrix(vgg,os.path.join('data','train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell hat also nur ~820 von 23.000 Bildern falsch klassifiziert (~3,6%), obwohl das Finetuning mit nur 200 Bildern erfolgte (nicht vergessen: Das Modell wurde mit den Bildern aus Imagnet vortrainiert!). Dieser Werte lässt sich sicher noch verbessern, wenn wir das Finetuning auch auf andere Layer ausweiten - wie im vorigen Abschnitt beschrieben."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8b9adcc5-3417-4888-9455-5aef75e5163a",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.11829268292682926,
        "y": 0.1021680216802168
       },
       "content": {
        "cell": "20bc8fad-7e51-4e62-b04d-60aa45ed40f4",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     },
     "theme": null
    }
   },
   "themes": {
    "default": "f6b1bd64-6a8a-49df-808e-fba1b5799bfb",
    "theme": {
     "f6b1bd64-6a8a-49df-808e-fba1b5799bfb": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "f6b1bd64-6a8a-49df-808e-fba1b5799bfb",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252.0,
         252.0,
         252.0
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68.0,
         68.0,
         68.0
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197.0,
         226.0,
         245.0
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43.0,
         126.0,
         184.0
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.0
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8.0
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6.0
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5.0
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4.0
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4.0
      }
     }
    }
   }
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6.0,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
