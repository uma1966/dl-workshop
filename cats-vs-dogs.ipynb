{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8b9adcc5-3417-4888-9455-5aef75e5163a"
    }
   },
   "source": [
    "# Computer Vision Workshop\n",
    "\n",
    "Dieses Tutorial zeigt, wie man mittels eines Neuronalen Netzwerks / Deep Learning einen Bild-Klassifizierer baut, der Katzenbilder von Hundebildern unterscheidet.\n",
    "\n",
    "Diese Aufgabenstellung kommt aus dem [\"Cats vs. Dogs\"](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) Wettbewerb der Website Kaggle.\n",
    "\n",
    "Basis bildet ein Convolutional Neuronal Network (CNN) namens \"VGG16\", welches auf Basis der Daten des [Imagenet Datasets](http://image-net.org/synset?wnid=n02084071) vortrainiert wurde. Das Modell wird durch Umkonfiguration und Re-Training so angepasst, dass es die gestellte Aufgabe lösen kann.\n",
    "\n",
    "Die Grundlagen zu diesem Workshop kommen aus dem Deep Learning MOOC [fast.ai](http://fast.ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "40b6e98a-bc3c-4dd6-ac74-5d7571d7313a"
    }
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "74cb107e-1859-441c-a32b-b01cd059e998"
    }
   },
   "source": [
    "Die Daten des Kaggle Wettbewerbs wurden schon vorbereitet und in der \"richtigen\" Struktur abgelegt.\n",
    "Das Verzeichnis `data` enthält die Trainings- und Validierungsdaten aus dem Dataset. Dabei sind die Bilder zu jeder zu erkennenden \"Klasse\" (Cats & Dogs in unserem Fall) in einem eigenen Unterverzeichnis abgelegt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! führt einen Shell-Befehl aus...\n",
    "!tree -d data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mal schauen, wieviele Dateien in den Trainings- und Validerungsdaten drin sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "20bc8fad-7e51-4e62-b04d-60aa45ed40f4"
    }
   },
   "outputs": [],
   "source": [
    "!echo -n \"Training cats: \" && ls data/train/cats | wc -w\n",
    "!echo -n \"Training dogs: \" && ls data/train/dogs | wc -w\n",
    "!echo -n \"Validation cats: \" && ls data/valid/cats | wc -w\n",
    "!echo -n \"Validation dogs: \" && ls data/valid/dogs | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Verzeichnis `test` enthält die Bilder, die nicht klassifizert sind (deshalb das Unterverzeichnis `unknown`). Diese wollen wir nach dem Training bestimmen. Mal sehen, wieviele das sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo -n \"Test: \" && ls data/test/unknown | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir am Code herumprobieren können, ohne gleich lange Laufzeiten aufgrund der vielen Dateien zu erhalten, gibt es noch ein `sample` Dataset, welches gleich aufgebaut ist, aber nur nur einen kleinen Teil der Daten enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tree -d sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo -n \"Training cats: \" && ls sample/train/cats | wc -w\n",
    "!echo -n \"Training dogs: \" && ls sample/train/dogs | wc -w\n",
    "!echo -n \"Validation cats: \" && ls sample/valid/cats | wc -w\n",
    "!echo -n \"Validation dogs: \" && ls sample/valid/dogs | wc -w\n",
    "!echo -n \"Test: \" && ls sample/test/unknown | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Diesen Teil immer ausführen. Hier werden notwendige Packete geladen und globale Variablen initialisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "import utils\n",
    "import keras\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier setzen wir den Pfad für die Daten, mit denen wir arbeiten wollen (also `data` oder `sample`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path = \"data/\"\n",
    "path = \"sample/\"\n",
    "path = os.path.join(os.path.curdir,path)\n",
    "print path\n",
    "\n",
    "train_path = os.path.join(path,\"train\")\n",
    "valid_path = os.path.join(path,\"valid\")\n",
    "test_path = os.path.join(path,\"test\")\n",
    "result_path = os.path.join(path,\"results\")\n",
    "\n",
    "print train_path\n",
    "print valid_path\n",
    "print test_path\n",
    "print result_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden die Python Klasse, welche das Modell (ein CNN) in ein nettes, mehr oder weniger objektorientiertes API verpackt. Der Sourcecode dazu steht in der Datei `vgg16.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=64\n",
    "\n",
    "# Wie viele Durchläufe durch die Trainingsdaten sollen gemacht werden:\n",
    "epochs = 1\n",
    "\n",
    "# Import VGG16 class, and instantiate\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden die Trainings- und Validierungsdaten als \"Batches\". `vgg.get_batches()` setzt voraus, dass die Daten in Unterverzeichnissen je Kategorie abgelegt sind. Genau das ist bei uns der Fall, wie wir oben gesehen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batches = vgg.get_batches(train_path, batch_size=batch_size)\n",
    "validation_batches = vgg.get_batches(valid_path, batch_size=batch_size*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann wird das VGG16 Modell an unsere Aufgabe (\"cat or dog\" Klassifizierung) angepasst: `vgg.finetune()`. Dies ändert die Architektur des Netzes: Der letzte Layer wird verworfen und durch einen neuen Layer erstetzt, welcher nur noch 2 Outputs hat (statt wie bisher 1000): Cats & Dogs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.finetune(train_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns anschauen, was die Methode `finetune` macht (`??` zeigt und die Definition eines Code-Elements):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg.finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der eigentliche Austausch des letzten Layers erfolgt in der Methode `vgg.ft()`. Die Gewichte des neuen Layers sind zunächst mit Zufallswerten initialisiert worden, d.h. sie müssen noch trainiert werden. Die anderen Layer lassen wir, wie sie sind (`layer.trainable = False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg.ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt trainieren wir das Modell mit den Daten über `vgg.fit()`. Dabei wird in Wahrheit nur noch der letze (modifizierte) Layer des angepassten VGG16 Modells trainiert. Die angepassten Gewichte des Modells schreiben wir in eine Datei, so dass wir sie später wieder laden können und so nicht jedesmal das Training wiederholen müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rate:\n",
    "vgg.model.optimizer.lr = 0.01\n",
    "\n",
    "print('start fitting at {}'.format(time.asctime()))\n",
    "\n",
    "vgg.fit(train_batches, validation_batches, nb_epoch=epochs)\n",
    "\n",
    "print('stop fitting at {}'.format(time.asctime()))\n",
    "weights_filename = os.path.join(result_path,'finetune1.h5')\n",
    "print 'saving weights to {}'.format(weights_filename)\n",
    "vgg.model.save_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage / Klassifizierung (Prediction)\n",
    "\n",
    "Jetzt klassifizieren wir mit dem re-trainierten Modell die Bilder, die im Unterverzeichnis 'test' abgelegt sind.\n",
    "\n",
    "Prediction Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_file = os.path.join(result_path,'predictions.dat')\n",
    "filenames_file = os.path.join(result_path,'filenames.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction durchführen und Ergebnisse speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('start predicting at {}'.format(time.asctime()))\n",
    "test_batches, predictions = vgg.test(test_path,batch_size=batch_size*2)\n",
    "print('stop predicting at {}'.format(time.asctime()))\n",
    "\n",
    "utils.save_array(predictions_file, predictions)\n",
    "utils.save_array(filenames_file, test_batches.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mal ein paar Ergebnisse anschauen....\n",
    "\n",
    "Zunächst eine kleine Hilfsmethode, um Bilder anzeigen zu können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def plots_idx(idx, path, filenames, titles=None):\n",
    "    utils.plots([image.load_img(os.path.join(path,filenames[i])) for i in idx], titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = utils.load_array(predictions_file)\n",
    "filenames = utils.load_array(filenames_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wählen zufällig ein paar Bilder aus und zeigen sie mit der Vorhersage an (`[Wahrscheinlichkeit Katze, Wahrscheinlichkeit Hund]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(test_batches.filenames),4)\n",
    "plots_idx(idx, path=test_path, filenames=test_batches.filenames, titles=predictions[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisieren der Vorhersage\n",
    "\n",
    "Wir wollen uns anschauen, wie gut unser Modell eigentlich vorhersagt. Die Idee dazu ist, dass wir mit dem Modell eine Vorhersage über die bereits klassifizierten Trainingsdaten machen. So kennen wir die \"ground truth\" zu jedem Bild und können ermitteln, ob die Vorhersage korrekt war."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die trainierten Gewichte werden in das Modell geladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren (falls wir hier wieder beginnen wollen):\n",
    "vgg = Vgg16()\n",
    "# Nicht vergessen, die letzte Schicht zu verändern (2 statt 1000 Klassen als Output)!\n",
    "vgg.finetune(train_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_filename = os.path.join(result_path,'finetune1.h5')\n",
    "vgg.model.load_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhersage mit den Validierungsdaten. So kennnen wir die \"ground truth\" und können sie mit der Vorhersage des Modells vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_batches, predictions = vgg.test(valid_path, batch_size=64)\n",
    "expected_labels = valid_batches.classes\n",
    "filenames = valid_batches.filenames\n",
    "\n",
    "print predictions[:8]\n",
    "\n",
    "our_predictions = predictions[:,0]\n",
    "our_labels = np.round(1-our_predictions)\n",
    "print our_labels[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeige einige korrekte Klassifizierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = np.where(our_labels==expected_labels)[0]\n",
    "print \"Found {} correct labels\".format(len(correct))\n",
    "idx = np.random.permutation(correct)[:4]\n",
    "plots_idx(idx, valid_path, filenames, our_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zeige einige falsche Klassifizierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incorrect = np.where(our_labels!=expected_labels)[0]\n",
    "print \"Found {} incorrect labels.\".format(len(incorrect))\n",
    "if len(incorrect)>0:\n",
    "    idx = np.random.permutation(incorrect)[:4]\n",
    "    plots_idx(idx, valid_path, filenames, our_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeige einige richtige Klassifizierungen mit der größten Wahrscheintlichkeit\n",
    "\n",
    "... also da, wo das Modell wirklich recht hatte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confident_cats = np.where((our_labels==expected_labels) & (our_labels==0))[0]\n",
    "print \"Found {} confident correct cats labels\".format(len(confident_cats))\n",
    "idx = np.random.permutation(confident_cats)[:4]\n",
    "plots_idx(idx,valid_path, filenames, our_labels[idx])\n",
    "\n",
    "confident_dogs = np.where((our_labels==expected_labels) & (our_labels==1))[0]\n",
    "print \"Found {} confident correct dogs labels\".format(len(confident_dogs))\n",
    "idx = np.random.permutation(confident_dogs)[:4]\n",
    "plots_idx(idx,valid_path, filenames, our_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ziege einige falsche Klassifizierungen mit der größten Wahrscheinlichkeit\n",
    "\n",
    "... also die, bei denen das Modell total daneben lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confident_cats = np.where((our_labels!=expected_labels) & (our_labels==0))[0]\n",
    "print \"Found {} confident incorrect cats labels\".format(len(confident_cats))\n",
    "if len(confident_cats)>0:\n",
    "    idx = np.random.permutation(confident_cats)[:4]\n",
    "    plots_idx(idx,valid_path, filenames, our_labels[idx])\n",
    "\n",
    "confident_dogs = np.where((our_labels!=expected_labels) & (our_labels==1))[0]\n",
    "print \"Found {} confident incorrect dogs labels\".format(len(confident_dogs))\n",
    "if len(confident_dogs)>0:\n",
    "    idx = np.random.permutation(confident_dogs)[:4]\n",
    "    plots_idx(idx,valid_path, filenames, our_labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ziege die unsichersten Klassifizierungen\n",
    "\n",
    "... also die, bei denen sich das Modell nicht so sicher war."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncertain = np.argsort(np.abs(our_predictions-0.5))\n",
    "plots_idx(uncertain[:6], valid_path, filenames, our_predictions[uncertain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels,our_labels)\n",
    "utils.plot_confusion_matrix(cm, valid_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mehr Layer trainieren (Backpropagation)\n",
    "\n",
    "Bisher haben wir nur den letzten Layer des Modells neu trainiert und den Rest des Modells nicht angetastet. Wenn wir nun auch die mittleren Layer re-trainieren wollen geht das mit Keras ziemlich einfach...\n",
    "\n",
    "Zunächst mal einen Blick auf die Architektur des Modells werfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren (falls wir hier wieder beginnen wollen):\n",
    "vgg = vgg16.Vgg16()\n",
    "# Nicht vergessen, die letzte Schicht zu verändern (2 statt 1000 Klassen als Output)!\n",
    "vgg.finetune(train_batches)\n",
    "\n",
    "weights_filename = os.path.join(result_path,'finetune1.h5')\n",
    "vgg.model.load_weights(weights_filename)\n",
    "\n",
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achtung: Wir müssen darauf achten, den letzten Layer (den wir oben selbst hinzugefügt haben) vorher über `vgg.fit()` auch trainiert zu haben, da er sonst mit Zufallswerten initialisert ist, welche das Training der Zwischenschichten ziemlich durcheinander bringen würde. Wenn wir die zuvor in einer Datei gespeicherten Gewichte verwenden ist das automatisch der Fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hilfsmethode, um Modell anzupassen (fitting):\n",
    "def fit_model(model, train_batches, validation_batches, nb_epoch=1):\n",
    "    model.fit_generator(train_batches, samples_per_epoch=train_batches.N, nb_epoch=nb_epoch, \n",
    "                        validation_data=validation_batches, nb_val_samples=validation_batches.N)\n",
    "    \n",
    "# Hole den Index des ersten \"dense\" layers:\n",
    "first_dense_idx = [index for index,layer in enumerate(vgg.model.layers) if type(layer) is keras.layers.core.Dense][0]\n",
    "print \"First dense layer is layer no. \" + str(first_dense_idx)\n",
    "# ...und setze diesen und alle nachfolgenden auf \"trainierbar\":\n",
    "for layer in vgg.model.layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt trainieren wir _alle_ Layer ab dem ersten \"Dense\" Layer neu und speichern die Gewichte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras.backend.set_value(vgg.model.optimizer.lr, 0.01)\n",
    "model_file = os.path.join(result_path,'finetune2.h5')\n",
    "\n",
    "fit_model(vgg.model, train_batches, validation_batches, 3)\n",
    "\n",
    "vgg.model.save_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.evaluate_generator(utils.get_batches(dirname=valid_path, shuffle=False, batch_size=batch_size*2), valid_batches.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Modell zu beurteilen, machen wir wieder eine Vorhersage über ein klassifziertes Dataset (\"ground truth\") und berechnen eine Cross Entropy Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_and_plot_confusion_matrix(vgg, path):\n",
    "    # Prediction:\n",
    "    batches, predictions = vgg.test(path, batch_size=64)\n",
    "    our_predictions = predictions[:,0]\n",
    "    our_labels = np.round(1-our_predictions)\n",
    "\n",
    "    # Ground truth:\n",
    "    expected_labels = batches.classes\n",
    "\n",
    "    cm = sklearn.metrics.confusion_matrix(expected_labels,our_labels)\n",
    "    utils.plot_confusion_matrix(cm, batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_and_plot_confusion_matrix(vgg,valid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können auch probieren, noch mehr Layer zu trainieren (nicht nur die Dense-Layer am hinteren Ende des Networks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in vgg.model.layers[12:]: layer.trainable=True\n",
    "keras.backend.set_value(vgg.model.optimizer.lr, 0.001)\n",
    "model_file = os.path.join(result_path,'finetune3.h5')\n",
    "\n",
    "fit_model(vgg.model, train_batches, validation_batches, 4)\n",
    "\n",
    "vgg.model.save_weights(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier berechnen wir wieder eine Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_and_plot_confusion_matrix(vgg,valid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exkurs: Wenige Bilder Trainieren, viele Bilder klassifizieren\n",
    "\n",
    "Wie gut ist unser Modell eigentlich, wenn wir nur mit wenigen Bildern trainieren?\n",
    "\n",
    "Vorgehensweise:\n",
    "* Trainieren mit den Bildern des `sample` Datasets (200 Bilder)\n",
    "* Vorhersagen mit den Bildern des normalen Datasets (23000 Trainingsbilder als Ground Truth)\n",
    "\n",
    "Da wir hier nicht das ganze Training wiederholen wollen, laden wir die Gewichte aus dem `sample` Pfad - das setzt voraus, dass man den Trainings-Code oben auch mal mit dem `sample` Dataset ausgeführt hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren (falls wir hier wieder beginnen wollen):\n",
    "vgg_small = vgg16.Vgg16()\n",
    "# Nicht vergessen, die letzte Schicht zu verändern (2 statt 1000 Klassen als Output)!\n",
    "vgg_small.finetune(train_batches)\n",
    "# Gewichte laden (aus dem 'sample' Pfad!)\n",
    "weights_filename = os.path.join('sample','results','finetune1.h5')\n",
    "print \"using weights \" + weights_filename\n",
    "vgg_small.model.load_weights(weights_filename)\n",
    "\n",
    "# Jetzt führen wir die Vorhersage mit dem \"normalen\" Dataset durch:\n",
    "predict_and_plot_confusion_matrix(vgg_small,os.path.join('data','train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell hat also nur ~820 von 23.000 Bildern falsch klassifiziert (~3,6%), obwohl das Finetuning mit nur 200 Bildern erfolgte (nicht vergessen: Das Modell wurde mit den Bildern aus Imagnet vortrainiert!). Dieser Werte lässt sich sicher noch verbessern, wenn wir das Finetuning auch auf andere Layer ausweiten - wie im vorigen Abschnitt beschrieben."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8b9adcc5-3417-4888-9455-5aef75e5163a",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.11829268292682926,
        "y": 0.1021680216802168
       },
       "content": {
        "cell": "20bc8fad-7e51-4e62-b04d-60aa45ed40f4",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     },
     "theme": null
    }
   },
   "themes": {
    "default": "f6b1bd64-6a8a-49df-808e-fba1b5799bfb",
    "theme": {
     "f6b1bd64-6a8a-49df-808e-fba1b5799bfb": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "f6b1bd64-6a8a-49df-808e-fba1b5799bfb",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
