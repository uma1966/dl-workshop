{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4e20e30c-2f10-4eb4-9111-4517cc4c568b"
    }
   },
   "source": [
    "![](cats-vs-dogs.jpg)\n",
    "\n",
    "Dieses Tutorial zeigt, wie man mittels eines Neuronalen Netzwerks / Deep Learning einen Bild-Klassifizierer baut, der Katzenbilder von Hundebildern unterscheidet.\n",
    "\n",
    "Die Grundlagen zu diesem Workshop kommen aus dem Deep Learning MOOC [fast.ai](http://fast.ai) (Version 1) von Jeremy Howard.\n",
    "\n",
    "## Inhalt\n",
    "\n",
    "1. [Einstieg](#Einstieg) - Worum geht es hier? Was ist Deep Learning?\n",
    "1. [Vorbereitung der Daten](#Vorbereitung-der-Daten) - Welche Daten verarbeiten wir?\n",
    "1. [Setup](#Setup) - Initialisierung der notwendigen Frameworks\n",
    "1. [Das VGG16 Modell](#Das-VGG16-Modell) - Laden des Modells\n",
    "1. [Finetuning & Training](#Finetuning-und-Training) - Training des Modells mit unseren Daten\n",
    "1. [Vorhersage](#Vorhersage) - Das trainierte Modell anwenden, um eine Vorhersage zu treffen\n",
    "1. [Beurteilung der Qualität](#Beurteilung-der-Qualität) - Wir schauen uns an, welche Ergebnisse das Modell liefert\n",
    "1. [Verbessern der Qualität](#Verbessern-der-Qualität) - Verschiedene Möglichkeiten, die Vorhersagequalität zu verbessern\n",
    "1. [Exkurs: Wenige Bilder Trainieren, viele Bilder klassifizieren](#Exkurs:-Wenige-Bilder-Trainieren,-viele-Bilder-klassifizieren)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "54343e35-42ce-4749-81f5-b832ad1b632a"
    }
   },
   "source": [
    "## Einstieg\n",
    "\n",
    "Zunächst mal: Wie ist der Begriff einzuordnen?\n",
    "![Deep Learning Einordnung](deep-learning-1.png)\n",
    "\n",
    "Das besondere an Deep Learning ist, dass es in der Lage ist, Characteristiken (\"Features\") selbstständig aus den Daten zu extrahieren:\n",
    "![Deep Learning Ansatz](santos-DeepLearning.png)\n",
    "(Quelle: https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_learning.html)\n",
    "\n",
    "Einige gängige Awendungsgebiete:\n",
    "\n",
    "* Sprache (NLP - Natural Language Processing)\n",
    "  * Spracherkennung (speech recognition)\n",
    "  * Textverständnis (natural-language understanding)\n",
    "  * Erzeugung von Text (natural-language generation)\n",
    "  * Erzeugung von Sprache (speech generation)\n",
    "  \n",
    "* Bilderkennung (Computer Vision)\n",
    "  * **Klassifizierung (object classification - z.B. Cats vs. Dogs)**\n",
    "  * Identfikation (object identification - z.B. Gesichtserkennung)\n",
    "  * Erkennung (object detection - z.B. )\n",
    "  * Analyse von Bewegungen (pose estimation, object tracking)\n",
    "  * Bild-/Videorekonstruktion\n",
    "  \n",
    "* Zeitreihen\n",
    "  * Mustererkennung (z.B. Schwingungen, physiolog. Daten, Finanzdaten)\n",
    "  * Interpolation, Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Hier geht es endlich los. Diesen Teil immer ausführen. Hier werden notwendige Packete geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import shutil\n",
    "import os.path\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "import utils\n",
    "import keras\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten für unsere Aufgabenstellung kommen aus dem [\"Cats vs. Dogs\"](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) Wettbewerb der Website Kaggle.\n",
    "\n",
    "Die Daten des Kaggle Wettbewerbs wurden schon vorbereitet und in der \"richtigen\" Struktur abgelegt.\n",
    "Das Verzeichnis `data` enthält die Trainings- und Validierungsdaten aus dem Dataset. Dabei sind die Bilder zu jeder zu erkennenden \"Klasse\" (Cats & Dogs in unserem Fall) in einem eigenen Unterverzeichnis abgelegt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "88e65de9-f699-4f43-a44e-977f2ec7f1a4"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "47e1a323-7d6b-4d22-9688-10a6d414560c"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! führt einen Shell-Befehl aus...\n",
    "!tree -d {DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Im Verzeichnis `results` werden die Ergebnisse unserer Experimente abgelegt. \n",
    "* Im Verzeichnis `train` sind die Trainingsdaten drin, unterteilt in `cats` und `dogs`.\n",
    "* Im Verzeichnis `valid` sind die Validerungsdaten für das Training (ebenfalls in `cats` und `dogs` unterteilt).\n",
    "\n",
    "Mal schauen, wie viel das so ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo -n \"Training cats: \" && ls {DATA_DIR}/train/cats | wc -w\n",
    "!echo -n \"Training dogs: \" && ls {DATA_DIR}/train/dogs | wc -w\n",
    "!echo -n \"Validation cats: \" && ls {DATA_DIR}/valid/cats | wc -w\n",
    "!echo -n \"Validation dogs: \" && ls {DATA_DIR}/valid/dogs | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Verzeichnis `test` enthält die Bilder, die nicht klassifizert sind (deshalb das Unterverzeichnis `unknown`). Diese wollen wir nach dem Training bestimmen. Mal sehen, wieviele das sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo -n \"Test: \" && ls {DATA_DIR}/test/unknown | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir am Code herumprobieren können, ohne gleich lange Laufzeiten aufgrund der vielen Dateien zu erhalten, erzeugen wir uns ein `sample` Dataset, welches gleich aufgebaut ist, aber nur nur einen kleinen Teil der Daten enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_DIR = 'sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil, os, random\n",
    "\n",
    "def init_dir(target_dir):\n",
    "    shutil.rmtree(target_dir,ignore_errors=True)\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "def copy_random_files(source_dir, target_dir, subdir, n_files):\n",
    "    source_dir = os.path.join(source_dir, subdir)\n",
    "    target_dir = os.path.join(target_dir, subdir)\n",
    "    init_dir(target_dir)\n",
    "    source_files = random.sample(\n",
    "        [os.path.join(source_dir,file) for file in os.listdir(source_dir) if file.endswith(\".jpg\")],n_files)\n",
    "    for source in source_files:\n",
    "        shutil.copy(source,target_dir)\n",
    "\n",
    "init_dir(os.path.join(SAMPLE_DIR,'results'))\n",
    "copy_random_files(DATA_DIR, SAMPLE_DIR, \"valid/cats\", 50)\n",
    "copy_random_files(DATA_DIR, SAMPLE_DIR, \"valid/dogs\", 50)\n",
    "copy_random_files(DATA_DIR, SAMPLE_DIR, \"train/cats\", 400)\n",
    "copy_random_files(DATA_DIR, SAMPLE_DIR, \"train/dogs\", 400)\n",
    "copy_random_files(DATA_DIR, SAMPLE_DIR, \"test/unknown\", 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tree -d {SAMPLE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo -n \"Training cats: \" && ls {SAMPLE_DIR}/train/cats | wc -w\n",
    "!echo -n \"Training dogs: \" && ls {SAMPLE_DIR}/train/dogs | wc -w\n",
    "!echo -n \"Validation cats: \" && ls {SAMPLE_DIR}/valid/cats | wc -w\n",
    "!echo -n \"Validation dogs: \" && ls {SAMPLE_DIR}/valid/dogs | wc -w\n",
    "!echo -n \"Test: \" && ls {SAMPLE_DIR}/test/unknown | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier setzen wir den Pfad für die Daten, mit denen wir arbeiten wollen (also `data` oder `sample`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = DATA_DIR\n",
    "# path = SAMPLE_DIR\n",
    "\n",
    "path = os.path.join(os.path.curdir,path)\n",
    "print path\n",
    "\n",
    "train_path = os.path.join(path,\"train\")\n",
    "valid_path = os.path.join(path,\"valid\")\n",
    "test_path = os.path.join(path,\"test\")\n",
    "result_path = os.path.join(path,\"results\")\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "print train_path\n",
    "print valid_path\n",
    "print test_path\n",
    "print result_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Werfen wir einen kurzen Blick in die Daten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def plots_n(path, n=4):\n",
    "    \"\"\"Loads and displays first n images with titles from path.\"\"\"\n",
    "    filenames = os.listdir(path)[:n]\n",
    "    utils.plots([image.load_img(os.path.join(path,filename)) for filename in filenames], titles=filenames)\n",
    "\n",
    "plots_n(os.path.join(train_path,'cats'))\n",
    "plots_n(os.path.join(train_path,'dogs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das VGG16 Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden Convolutional Neuronal Network (CNN) namens \"VGG16\". Entwickelt wurde [VGG16 im Jahr 2014](https://arxiv.org/pdf/1409.1556.pdf) von der Vision Geometry Group an der University of Oxford  und mit den Daten des [Imagenet Datasets](http://image-net.org/synset?wnid=n02084071) vortrainiert. Es erkennt 1000 verschiedene Ojekte, d.h. es liefert zu einem Bild 1000 Wahrscheinlichkeitswerte, ob das Bild ein Ding dieser Klasse enthält.\n",
    "\n",
    "![VGG16](vgg16.png) Quelle: http://www.datalearner.com/paper_note/content/300035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden ein Python Modul, welches VGG16 in ein nettes, mehr oder weniger objektorientiertes API verpackt. Der Sourcecode dazu steht in der Datei `vgg16.py`. Der Code stammt - wie oben erwähnt - von [fast.ai](http://fast.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import vgg16\n",
    "vgg = vgg16.Vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können einen Blick in den Code der Klasse werfen (?? zeigt die Implementierung eines Code-Elements):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können auch einen Blick auf die Struktur des Modells werfen. Der folgende Befehl zeigt die Schichten (\"Layer\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Number of classes: \", len(vgg.classes)\n",
    "print \"First n classes:\"\n",
    "vgg.classes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Frage, was so ein CNN eigentlich \"sieht\", ist gar nicht so leicht zu beantworten. Es gibt Ansätze, dies zu visualisieren. Die folgende Darstellung zeigt Schematisch das Funktionsprinzip.\n",
    "\n",
    "![Was ein CNN sieht](salzberg-deep-learning.png)\n",
    "(Quelle: http://genome.fieldofscience.com/2017/12/no-google-didnt-just-create-ai-that.html)\n",
    "\n",
    "Das CNN lernt, einfache Muster aus dem Bild zu extrahieren und diese zu immer komplexen Features zu kombinieren. Dazu extrahiert es wichtige Bildeigenschaften mit vielen 3x3 großen \"**Kernel**\".Das Prinzip der Kernel ist hier erläutert: http://setosa.io/ev/image-kernels. \n",
    "\n",
    "Das Netz selbst besteht aus hintereinandergeschalteten Convolution-Schichten, die immer abstraktare Features extrahieren. Die Werte der Kernel sind Gewichte, die beim Training zielgerichtet angepasst werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning und Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Schritt passen wir das Modell so an, dass es statt der 1000 Klassen nur noch die uns hier interssierenden 2 Klassen vorhersagt: Cats & Dogs.\n",
    "\n",
    "Dazu benötigen wir Finetuning & Training.\n",
    "\n",
    "Wir laden zunächst die Trainings- und Validierungsdaten als \"Batches\". Ein Batch liefert immer die nächsten _n_ Datensätze (`batch_size`) inklusive der Kategorie. Beim Training wird Batch für Batch über die gesamte Menge der Trainings- und Validierungsdaten iteriert. `batch_size` kann man so groß machen wie möglich aber die Beschränkung ist das verfügbare Memory der GPU. 64 ist eine gute Empfehlung für die Obergrenze, eventuell muss man weniger angeben.\n",
    "\n",
    "`vgg.get_batches()` erwartet, dass die Daten in Unterverzeichnissen je Kategorie abgelegt sind. Genau das ist bei uns der Fall, wie wir oben gesehen haben (Verzeichnisse \"cats\" und \"dogs\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_batches = vgg.get_batches(train_path, batch_size=batch_size)\n",
    "validation_batches = vgg.get_batches(valid_path, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir auch sehen, hat der Befehl automatisch erkannt, wie viele Klassen in unseren Trainings-/Validierungsdaten enthalten sind: `2 classes` (cats & cogs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann wird das VGG16 Modell an unsere Aufgabe (\"cat or dog\" Klassifizierung) angepasst mit `vgg.finetune()`. Dies ändert die Architektur des Netzes: Der letzte Layer wird verworfen und durch einen neuen Layer erstetzt, welcher nur noch 2 Outputs hat (statt wie bisher 1000): Cats & Dogs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.finetune(train_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns anschauen, was die Methode `finetune` macht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg.finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der eigentliche Austausch des letzten Layers erfolgt in der Methode `vgg.ft()`. Die Gewichte des neuen Layers sind zunächst mit Zufallswerten initialisiert worden, d.h. sie müssen noch trainiert werden. Die anderen Layer lassen wir, wie sie sind (`layer.trainable = False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg.ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt sind wir soweit, das Modell neu zu trainieren.\n",
    "\n",
    "![Training](training.png)\n",
    "\n",
    "Dabei wird in Wahrheit nur noch der letze (modifizierte) Layer des angepassten VGG16 Modells trainiert. Das Training wird mit `vgg.fit()` gestartet. Die \"learning rate\" bestimmt dabei, in welcher Schrittweite die Gewichte angepasst werden, `nb_epoch` bestimmt wie oft wir über die gesamte Menge der Trainingsdaten iterieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Learning rate:\n",
    "vgg.model.optimizer.lr = 0.01\n",
    "\n",
    "vgg.fit(train_batches, validation_batches, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am Ende eines Durchgangs durch alle Trainingsbilder (\"Epoch\") erhalten wir ein paar wichtige Werte:\n",
    "* `val_acc` \"Validation Accuracy\" sagt, wie gut die Validierungsdaten erkannt wurden\n",
    "* `val_loss` \"Validation Loss\" ist der entsprechende Fehler bezogen auf die Validerungsdaten\n",
    "* `acc` \"Training Accuracy\" sagt, wie gut die Trainingsdaten erkannt wurden\n",
    "* `loss` \"Training Loss\" ist der entsprechende Fehler bezogen auf die Trainingsdaten\n",
    "\n",
    "Unser Training Loss ist kleiner als der Validation Loss: Unser Modell macht (etwas) \"underfitting\". Das ist bei VGG16 typisch und so gewollt.\n",
    "\n",
    "Wäre dagegen unsere Validation Accuracy deutlich kleiner als die Training Accuracy, würde das \"overfitting\" bedeuten: Unser Modell hätte quasi nur die Trainingsdaten auswendig gelernt und nicht gut generalisiert und würde neue Daten schlechter Vorhersagen.\n",
    "\n",
    "Die angepassten Gewichte des Modells schreiben wir in eine Datei, so dass wir sie später wieder laden können und so nicht jedesmal das Training wiederholen müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_filename = os.path.join(result_path,'finetune1.h5')\n",
    "print 'saving weights to ' + weights_filename\n",
    "vgg.model.save_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage\n",
    "\n",
    "![Prediction](prediction.png)\n",
    "\n",
    "Nun wenden wir das anpepasste Modell an und klassifizieren wir die Bilder, die im Unterverzeichnis 'test' abgelegt sind. Anders ausgedrückt: Wir sagen für eine Menge Daten (= ein Bild) vorher, mit jeweils welcher Wahrscheinlichkeit diese Daten eine Katze bzw. ein Hund ist. Die Ergebnisse speichern wir wieder in Dateien, damit wir später bei Bedarf darauf zugreifen können, ohne das ganze Training wiederholen zu müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_batches, predictions = vgg.test(test_path,batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnisse speichern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dateien zum Speichern der Ergebnisse:\n",
    "predictions_file = os.path.join(result_path,'predictions.dat')\n",
    "filenames_file = os.path.join(result_path,'filenames.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = test_batches.filenames\n",
    "utils.save_array(predictions_file, predictions)\n",
    "utils.save_array(filenames_file, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mal ein paar Ergebnisse anschauen\n",
    "\n",
    "Zunächst eine kleine Hilfsmethode, um Bilder anzeigen zu können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def plots_idx(idx, path, filenames, titles=None):\n",
    "    \"\"\"Loads and displays images with given titles. The images are given with their index in filenames.\"\"\"\n",
    "    utils.plots([image.load_img(os.path.join(path,filenames[i])) for i in idx], titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden die Ergebnisse aus den oben geschriebenen Dateien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = utils.load_array(predictions_file)\n",
    "filenames = utils.load_array(filenames_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wählen zufällig ein paar Bilder aus und zeigen sie mit der Vorhersage an (`[Wahrscheinlichkeit_Katze, Wahrscheinlichkeit_Hund]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(len(test_batches.filenames)),4,replace=False)\n",
    "plots_idx(idx, path=test_path, filenames=test_batches.filenames, titles=predictions[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beurteilung der Qualität\n",
    "\n",
    "Wir wollen uns anschauen, wie gut unser Modell eigentlich vorhersagt. Die Idee dazu ist, dass wir mit dem Modell eine Vorhersage über die bereits klassifizierten Validierungsdaten machen. So kennen wir die \"ground truth\" zu jedem Bild und können ermitteln, ob die Vorhersage korrekt war."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den folgenden Block muss man nur ausführen, wenn man das Training oben bereits vorher mal gemacht hatte und die Gewichte in eine Datei geschrieben hat. Dann kann man hier direkt das Modell neu laden mit den (veränderten) Gewichten. Hat man in der gleichen Sitzung das Training schon gemacht, kann man diesen Schritt überspringen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren (falls wir hier wieder beginnen wollen):\n",
    "import vgg16\n",
    "vgg = vgg16.Vgg16()\n",
    "# Nicht vergessen, die letzte Schicht zu verändern (2 statt 1000 Klassen als Output)!\n",
    "vgg.finetune(train_batches)\n",
    "# Gewichte laden:\n",
    "weights_filename = os.path.join(result_path,'finetune1.h5')\n",
    "vgg.model.load_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhersage mit den Validierungsdaten. So kennnen wir die \"ground truth\" und können sie mit der Vorhersage des Modells vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Vorhersage machen:\n",
    "valid_batches, predictions = vgg.test(valid_path, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir stutzen uns das Ergebnis zurecht, so dass wir nur noch ein 1-dimeansionales Array mit einer `1` für einen vorhergesagten Hund und einer `0` für eine vorgesagte Katze haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"First n predictions:\"\n",
    "print predictions[:8]\n",
    "our_predictions = predictions[:,1]\n",
    "print \"Probabilities, if it's a dog: \", our_predictions[:8]\n",
    "our_labels = np.round(our_predictions)\n",
    "print \"Label, if it's a dog: \", our_labels[:8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist unsere \"ground truth\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_labels = valid_batches.classes\n",
    "filenames = valid_batches.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeige einige korrekte Klassifizierungen\n",
    "\n",
    "Mit Numpy (`np`) können wir wir den Index aller Bilder ermitteln, bei denen unser vorhergesagtes Label (0 oder 1) mit dem erwarteten Label der \"groud truth\" übereinstimmt. Davon wählen wir zufällig 4 aus und zeigen sie an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = np.where(our_labels==expected_labels)[0]\n",
    "print \"Found {} correct labels\".format(len(correct))\n",
    "idx = np.random.permutation(correct)[:4]\n",
    "titles = [filenames[i]+'\\n'+ str(our_labels[i]) for i in idx]\n",
    "plots_idx(idx, valid_path, filenames, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zeige einige falsche Klassifizierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incorrect = np.where(our_labels!=expected_labels)[0]\n",
    "print \"Found {} incorrect labels.\".format(len(incorrect))\n",
    "if len(incorrect)>0:\n",
    "    idx = np.random.permutation(incorrect)[:4]\n",
    "    titles = [filenames[i]+'\\n'+ str(our_labels[i]) for i in idx]\n",
    "    plots_idx(idx, valid_path, filenames, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeige einige richtige Klassifizierungen mit großer  Wahrscheinlichkeit\n",
    "\n",
    "... also die, bei denen das Modell wirklich recht hatte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_view = 4\n",
    "\n",
    "# The images we most confident were dogs, and are actually dogs\n",
    "correct_dogs = np.where((our_labels==1) & (our_labels==expected_labels))[0]\n",
    "print \"Found {} confident correct dogs labels\".format(len(correct_dogs))\n",
    "most_correct_dogs = np.argsort(our_predictions[correct_dogs])[::-1][:n_view]\n",
    "plots_idx(correct_dogs[most_correct_dogs], valid_path, filenames, our_predictions[correct_dogs][most_correct_dogs])\n",
    "\n",
    "# The images we most confident were cats, and are actually cats\n",
    "correct_cats = np.where((our_labels==0) & (our_labels==expected_labels))[0]\n",
    "print \"Found {} confident correct cats labels\".format(len(correct_cats))\n",
    "most_correct_cats = np.argsort(our_predictions[correct_cats])[::][:n_view]\n",
    "plots_idx(correct_cats[most_correct_cats], valid_path, filenames, our_predictions[correct_cats][most_correct_cats])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeige einige falsche Klassifizierungen mit großer Wahrscheinlichkeit\n",
    "\n",
    "... also die, bei denen das Modell total daneben lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The images we most confident were dogs, and are actually dogs\n",
    "confident_dogs = np.where((our_labels==1) & (our_labels!=expected_labels))[0]\n",
    "print \"Found {} confident incorrect dogs labels\".format(len(confident_dogs))\n",
    "if len(confident_dogs)>0:\n",
    "    most_confident_dogs = np.argsort(our_predictions[confident_dogs])[::-1][:n_view]\n",
    "    plots_idx(confident_dogs[most_confident_dogs], valid_path, filenames, our_predictions[confident_dogs][most_confident_dogs])\n",
    "\n",
    "# The images we most confident were dogs, and are actually dogs\n",
    "confident_cats = np.where((our_labels==0) & (our_labels!=expected_labels))[0]\n",
    "print \"Found {} confident incorrect cats labels\".format(len(confident_cats))\n",
    "if len(confident_cats)>0:\n",
    "    most_confident_cats = np.argsort(our_predictions[confident_cats])[::][:n_view]\n",
    "    plots_idx(confident_cats[most_confident_cats], valid_path, filenames, our_predictions[confident_cats][most_confident_cats])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ziege die unsichersten Klassifizierungen\n",
    "\n",
    "... also die, bei denen sich das Modell nicht so sicher war."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncertain = np.argsort(np.abs(our_predictions-0.5))\n",
    "titles = [str(our_predictions[i])+'\\n'+filenames[i] for i in uncertain]\n",
    "plots_idx(uncertain[:6], valid_path, filenames, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels,our_labels)\n",
    "utils.plot_confusion_matrix(cm, valid_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbesserung der Qualität\n",
    "\n",
    "Wollen wir die Qualität unseres Modells weiter verbessern haben wir mehrere Möglichkeiten:\n",
    "* mehr Trainingsdaten verwenden\n",
    "* mit den vorhandenen Trainingsdaten mehrmals trainieren\n",
    "* die vorhandenen Trainingsdaten leicht verändern (data augmentation)\n",
    "* mehr Layer trainieren\n",
    "* die Lernrate verändern\n",
    "\n",
    "Bisher haben wir nur den letzten Layer des Modells neu trainiert und den Rest des Modells nicht angetastet. Wenn wir nun auch die mittleren Layer re-trainieren wollen geht das mit Keras ziemlich einfach...\n",
    "\n",
    "Zunächst: Modell laden und initalisieren, indem wir die Gewichte aus unserem Training oben laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_model(batches, weights):\n",
    "    # Modell nochmal neu initialisieren:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    # Nicht vergessen, die letzte Schicht zu verändern (n statt 1000 Klassen als Output)!\n",
    "    vgg.finetune(batches)\n",
    "    # Gewichte aus Datei laden:\n",
    "    weights_filename = os.path.join(result_path, weights)\n",
    "    vgg.model.load_weights(weights_filename)\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg2 = init_model(train_batches, 'finetune1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nochmal kurz einen Blick auf die Struktur des Modells werfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg2.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achtung: Wir müssen darauf achten, den letzten Layer (den wir oben selbst hinzugefügt haben) vorher über `vgg.fit()` auch trainiert zu haben, da er sonst mit Zufallswerten initialisert ist, welche das Training der Zwischenschichten ziemlich durcheinander bringen würde. Wenn wir die zuvor in einer Datei gespeicherten Gewichte verwenden ist das automatisch der Fall.\n",
    "\n",
    "\n",
    "### Alle Dense Layer neu trainieren\n",
    "Der erste Versuch ist, nur die `Dense` Layer am Ende des Modells neu zu trainieren. `Dense` Layer bilden Lineare Funktionen ab, die mit allen Outputs der vorigen Layer verbunden sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hilfsmethode, um Modell anzupassen (fitting):\n",
    "def fit_model(model, train_batches, validation_batches, nb_epoch=1):\n",
    "    model.fit_generator(train_batches, samples_per_epoch=train_batches.N, nb_epoch=nb_epoch, \n",
    "                        validation_data=validation_batches, nb_val_samples=validation_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hole den Index des ersten \"dense\" layers:\n",
    "first_dense_idx = [index for index,layer in enumerate(vgg2.model.layers) if type(layer) is keras.layers.core.Dense][0]\n",
    "print \"First dense layer is layer no. \" + str(first_dense_idx)\n",
    "# ...und setze diesen und alle nachfolgenden auf \"trainierbar\":\n",
    "for layer in vgg2.model.layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt trainieren wir _alle_ Layer ab dem ersten \"Dense\" Layer neu (diesmal mit 3 Durchläufen durch die Trainingsdaten) und speichern die Gewichte wieder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.backend.set_value(vgg2.model.optimizer.lr, 0.01)\n",
    "\n",
    "fit_model(vgg2.model, train_batches, validation_batches, 3)\n",
    "\n",
    "weights_filename = os.path.join(result_path,'finetune2.h5')\n",
    "vgg2.model.save_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Modell zu beurteilen, ist es sinnvoll, die \"Categorical Cross Entropy\" zu berechnen und anzuzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_and_plot_confusion_matrix(vgg, path):\n",
    "    # Prediction:\n",
    "    batches, predictions = vgg.test(path, batch_size=64)\n",
    "    our_predictions = predictions[:,0]\n",
    "    our_labels = np.round(1-our_predictions)\n",
    "\n",
    "    # Ground truth:\n",
    "    expected_labels = batches.classes\n",
    "\n",
    "    cm = sklearn.metrics.confusion_matrix(expected_labels,our_labels)\n",
    "    utils.plot_confusion_matrix(cm, batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir machen wieder eine Vorhersage über ein klassifziertes Dataset (\"ground truth\") und zeigen die Categorcal Cross Entropy an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_and_plot_confusion_matrix(vgg2,valid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noch mehr Layer trainieren\n",
    "\n",
    "Wir können auch versuchen, noch mehr Layer zu trainieren (nicht nur die Dense-Layer am hinteren Ende des Networks). Ausserdem kann man auch mit der Learning Rate experimentieren.\n",
    "\n",
    "(Kleine Anmerkung dazu: In unserem Fall bringt das wahrscheinlich nicht mehr viel, da unsere Trainingsbilder bereits sehr ähnlich zu den Imagenet-Bildern sind, mit denen das Netz ursprünglich trainiert wurde. Es ist also zu erwarten, dass die vorderen Layer bereits recht gut die relevanten primitiven Eigenchaften erkennen.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren:\n",
    "vgg3 = init_model(train_batches,'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in vgg3.model.layers[12:]: \n",
    "    layer.trainable=True\n",
    "\n",
    "keras.backend.set_value(vgg3.model.optimizer.lr, 0.01)\n",
    "\n",
    "fit_model(vgg3.model, train_batches, validation_batches, 3)\n",
    "\n",
    "model_file = os.path.join(result_path,'finetune3.h5')\n",
    "vgg3.model.save_weights(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier berechnen wir wieder eine Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_and_plot_confusion_matrix(vgg3,valid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exkurs: Wenige Bilder Trainieren, viele Bilder klassifizieren\n",
    "\n",
    "Wie gut ist unser Modell eigentlich, wenn wir nur mit wenigen Bildern trainieren?\n",
    "\n",
    "Vorgehensweise:\n",
    "* Trainieren mit den Bildern des `sample` Datasets (200 Bilder)\n",
    "* Vorhersagen mit den Bildern des normalen Datasets (23000 Trainingsbilder als Ground Truth)\n",
    "\n",
    "Da wir hier nicht das ganze Training wiederholen wollen, laden wir die Gewichte aus dem `sample` Pfad - das setzt voraus, dass man den Trainings-Code oben auch mal mit dem `sample` Dataset ausgeführt hat und somit die Datei mit den Gewichten existiert. Im Code unten kann man festlegen, welche Gewichte wir verwenden wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modell nochmal neu initialisieren\n",
    "vgg = vgg16.Vgg16()\n",
    "# Nicht vergessen, die letzte Schicht zu verändern (2 statt 1000 Klassen als Output)!\n",
    "vgg.finetune(train_batches)\n",
    "\n",
    "# Gewichte laden (aus dem 'sample' Pfad!)\n",
    "weights = 'finetune1.h5'   # Nur der letzte Layer wurde trainiert\n",
    "#weights = 'finetune2.h5'   # Alle Dense Layer wurden trainiert\n",
    "#weights = 'finetune3.h5'   # Alle Layer ab Nr. 12 wurden trainiert\n",
    "weights = os.path.join('sample','results',weights)\n",
    "\n",
    "print \"using weights \" + weights\n",
    "vgg.model.load_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Jetzt führen wir die Vorhersage mit dem \"normalen\" Dataset durch:\n",
    "predict_and_plot_confusion_matrix(vgg, os.path.join('data','train'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "0e5cc8f1-6ed6-4f16-8f51-02d2578b3641": {
     "id": "0e5cc8f1-6ed6-4f16-8f51-02d2578b3641",
     "layout": "manual",
     "prev": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "regions": {
      "16d34e86-5380-4b3a-ad57-86399dcbcca1": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "47e1a323-7d6b-4d22-9688-10a6d414560c",
        "part": "whole"
       },
       "id": "16d34e86-5380-4b3a-ad57-86399dcbcca1"
      },
      "60742fda-df63-475e-a776-c7088256c241": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.0019292604501607534,
        "y": 0.12286530903894251
       },
       "id": "60742fda-df63-475e-a776-c7088256c241"
      }
     }
    },
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4e20e30c-2f10-4eb4-9111-4517cc4c568b",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.11829268292682926,
        "y": 0.1021680216802168
       },
       "content": {
        "cell": "54343e35-42ce-4749-81f5-b832ad1b632a",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     },
     "theme": null
    }
   },
   "themes": {
    "default": "f6b1bd64-6a8a-49df-808e-fba1b5799bfb",
    "theme": {
     "f6b1bd64-6a8a-49df-808e-fba1b5799bfb": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "f6b1bd64-6a8a-49df-808e-fba1b5799bfb",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
